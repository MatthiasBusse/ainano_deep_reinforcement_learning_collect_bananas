{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Navigation by deep reinforcement learning\n",
    "---\n"
   ]
  },
  {
   "source": [
    "# Load the environment, brains and print some info\n",
    "\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "file_name=\"Banana_Linux_NoVis/Banana.x86_64\")\n",
    "```"
   ]
  },
  {
   "source": [
    "file_name=\"Banana_Linux_NoVis/Banana.x86_64\""
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n"
   ]
  },
  {
   "source": [
    "## Brains\n",
    "\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment\n",
    "env = UnityEnvironment(file_name)\n",
    "\n",
    "# get brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n"
   ]
  },
  {
   "source": [
    "# Choose hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 2000         # number of episodes to train the agent \n",
    "LR = 5e-4               # learning rate\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "MAX_T = 1000            # timesteps per episode\n",
    "EPS_START = 1.0         # epsilon (greedy policy) start value\n",
    "EPS_END = 0.01          # epsilon minimal value\n",
    "EPS_DECAY = 0.995       # epsilon decay rate (per episode)"
   ]
  },
  {
   "source": [
    "# Train the agent "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from dqn_learning import dqn_learning\n",
    "\n",
    "# train\n",
    "training_agent = Agent(state_size=brain.vector_observation_space_size,\n",
    "              action_size=brain.vector_action_space_size,\n",
    "              seed=0, gamma=GAMMA, tau=TAU,\n",
    "              lr=LR, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, update_every=UPDATE_EVERY)\n",
    "\n",
    "scores = dqn_learning(env, training_agent, n_episodes=N_EPISODES, max_t=MAX_T,\n",
    "                        eps_start=EPS_START, eps_end=EPS_END, eps_decay=EPS_DECAY)\n",
    "\n",
    "# close environment\n",
    "env.close()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "# Save the model weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(training_agent.qnetwork_local.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "source": [
    "# Let the trained agent play and collect bananas. See its score\n",
    "Make sure to:\n",
    "\n",
    "1. clear outputs and restart the kernel \n",
    "2. rerun the top cells including loading the environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Load an experienced agent for playing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playing_agent import Playing_Agent\n",
    "# load playing agent\n",
    "playing_agent = Playing_Agent()\n"
   ]
  },
  {
   "source": [
    "## 2. Play"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set non train mode\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# initialize score \n",
    "score = 0                                      \n",
    "\n",
    "# play\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "while True:\n",
    "    action = playing_agent.act(state)              # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # next state\n",
    "   \n",
    "    if done:                                       \n",
    "        break\n",
    "    \n",
    "# print score when episode has finished\n",
    "\n",
    "print(\"\\nScore: {}\".format(score))"
   ]
  },
  {
   "source": [
    "## 3. Make sure to close the environment after playing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}